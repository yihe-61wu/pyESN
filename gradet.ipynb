{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo State Network as a tunable frequency generator\n",
    "\n",
    "This is a simplified implementation of Herbert Jaeger's task of learning a simple non-autonomous system, [a frequency generator controlled by an external signal](http://www.scholarpedia.org/article/Echo_state_network). Plots at the end.\n",
    "\n",
    "[See the ESN implementation](https://github.com/cknd/pyESN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from pyESN import ESN\n",
    "from signalgen import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "The network will learn to generate a wave signal whose frequency is determined by some slowly changing control input.\n",
    "\n",
    "#### 1) Generate some sample data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# updown signal\n",
    "N = 15000 # signal length\n",
    "n_changepoints = int(N/200)\n",
    "x_ud = updown_generator(N, n_changepoints, rng)\n",
    "\n",
    "# sine wave\n",
    "x_sn = np.vstack((np.sin(np.arange(N)/np.pi/10), np.ones(N))).T\n",
    "x = np.multiply(x_ud, x_sn)\n",
    "\n",
    "max_order = 3\n",
    "antiderivative = True\n",
    "y = signal1d_derivatives(x[:, 0], max_order, antiderivative)\n",
    "y[:,-1] = y[:,-1] / 30 + 2 \n",
    "\n",
    "traintest_cutoff = int(np.ceil(0.7*N))\n",
    "\n",
    "train_ctrl,train_output = x[:traintest_cutoff], y[:traintest_cutoff]\n",
    "test_ctrl, test_output  = x[traintest_cutoff:], y[traintest_cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Instantiate, train & test the network\n",
    "Parameters are mostly the same as in Herbert Jaeger's original Matlab code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harvesting states...\n",
      "fitting...\n",
      "training error:\n",
      "0.008491444714293299\n",
      "test error:\n",
      "0.4051654726982421\n"
     ]
    }
   ],
   "source": [
    "n_outputs = max_order + 1 + antiderivative\n",
    "\n",
    "esn = ESN(n_inputs = 2,\n",
    "          n_outputs = n_outputs,\n",
    "          n_reservoir = 200,\n",
    "          spectral_radius = 0.1,\n",
    "          sparsity = 0.9,\n",
    "          noise = 0.001,\n",
    "          input_shift = [-.5,-.5],\n",
    "          input_scaling = [2, 2],\n",
    "          teacher_scaling = .2,\n",
    "          teacher_shift = 0,\n",
    "          out_activation = np.tanh,\n",
    "          inverse_out_activation = np.arctanh,\n",
    "          random_state = rng,\n",
    "          silent = False)\n",
    "\n",
    "pred_train = esn.fit(train_ctrl, train_output)\n",
    "\n",
    "print(\"test error:\")\n",
    "pred_test = esn.predict(test_ctrl)\n",
    "print(np.sqrt(np.mean((pred_test - test_output)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Plots\n",
    "First, a look at the control signal, the target signal and the output of the model both during training and during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'training (excerpt)')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_tr = range(int(len(train_output)/4),int(len(train_output)/4+2000))\n",
    "fig, axes = plt.subplots(n_outputs, 2, figsize=(20,10))\n",
    "for axrow, y, yhat in zip(axes, train_output.T, pred_train.T):\n",
    "    axrow[0].plot(y[window_tr],label='target')\n",
    "    axrow[0].plot(yhat[window_tr],label='model')\n",
    "    axrow[0].legend(fontsize='x-small')\n",
    "    axrow[1].plot(abs(y[window_tr]-yhat[window_tr]))\n",
    "axes[0,0].set_title('training (excerpt)')\n",
    "axes[0,1].set_title('absolute error')\n",
    "\n",
    "window_test = range(2000)\n",
    "fig, axes = plt.subplots(n_outputs, 2, figsize=(20,10))\n",
    "for axrow, y, yhat in zip(axes, test_output.T, pred_test.T):\n",
    "    axrow[0].plot(y[window_test],label='target')\n",
    "    axrow[0].plot(yhat[window_test],label='model')\n",
    "    axrow[0].legend(fontsize='x-small')\n",
    "    axrow[1].plot(abs(y[window_test]-yhat[window_test]))\n",
    "axes[0,0].set_title('training (excerpt)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
